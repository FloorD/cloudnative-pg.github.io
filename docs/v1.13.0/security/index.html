<!DOCTYPE html>




<html lang="en">

<head>
  <title>CloudNative PG</title>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <meta name="description" content="CloudNativePG is the Kubernetes operator that covers the full lifecycle of a highly available PostgreSQL database cluster with a primary/standby architecture, using native streaming replication." />
  <meta name="keywords" content="" />
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet" />
  <link href="/main.min.css" rel="stylesheet" />
</head>

<body>
  <div class="h-screen w-screen">
    
<main>
  <header>
    <div class="flex flex-row px-16 justify-between py-5">
      <div>
        
        <a href="/"><img src="/logo/large_logo.svg" class="h-14" alt="Cloud Native Postgres Logo"></a>
      </div>
      <div>
        <ul class="flex gap-14 text-2xl text-slate-400 my-5">
          <li><a href="/docs/1.15.0/" target="_blank">Documentation</a></li>
          <li>Support</li>
          <li>End users</li>
          <li>Blog</li>
        </ul>
      </div>
      <div class="flex gap-5 my-4">
        <a href="https://github.com/cloudnative-pg/cloudnative-pg">
          <img src="/icons/Git.svg" alt="Github">
        </a>
        <a href="https://cloudnativepg.slack.com/">
          <img src="/icons/Slack.svg" alt="Slack">
        </a>
        <a href="https://twitter.com/CloudNativePg">
          <img src="/icons/Twitter.svg" alt="Twitter">
        </a>
        <a href="https://www.youtube.com/channel/UCTGH88W1BiuRRPTzJUDPJyA">
          <img src="/icons/YouTube.svg" alt="YouTube">
        </a>
      </div>
    </div>
  </header>
</main>


    
<h2 class="text-2xl"></h2>
<h1 id="security">Security</h1>
<p>This section contains information about security for Cloud Native PostgreSQL,
that are analyzed at 3 different layers: Code, Container and Cluster.</p>
<p>!!! Warning
The information contained in this page must not exonerate you from
performing regular InfoSec duties on your Kubernetes cluster. Please
familiarize yourself with the <a href="https://kubernetes.io/docs/concepts/security/overview/">&ldquo;Overview of Cloud Native Security&rdquo;</a>
page from the Kubernetes documentation.</p>
<p>!!! Seealso &ldquo;About the 4C&rsquo;s Security Model&rdquo;
Please refer to <a href="https://www.enterprisedb.com/blog/4cs-security-model-kubernetes">&ldquo;The 4Câ€™s Security Model in Kubernetes&rdquo;</a>
blog article to get a better understanding and context of the approach EDB
has taken with security in Cloud Native PostgreSQL.</p>
<h2 id="code">Code</h2>
<p>Source code of Cloud Native PostgreSQL is <em>systematically scanned</em> for static analysis purposes,
including <strong>security problems</strong>, using a popular open-source linter for Go called
<a href="https://github.com/golangci/golangci-lint">GolangCI-Lint</a> directly in the CI/CD pipeline.
GolangCI-Lint can run several <em>linters</em> on the same source code.</p>
<p>One of these is <a href="https://github.com/securego/gosec">Golang Security Checker</a>, or simply <code>gosec</code>,
a linter that scans the abstract syntactic tree of the source against a set of rules aimed at
the discovery of well-known vulnerabilities, threats, and weaknesses hidden in
the code such as hard-coded credentials, integer overflows and SQL injections - to name a few.</p>
<p>!!! Important
A failure in the static code analysis phase of the CI/CD pipeline is a blocker
for the entire delivery of Cloud Native PostgreSQL, meaning that each commit is validated
against all the linters defined by GolangCI-Lint.</p>
<h2 id="container">Container</h2>
<p>Every container image that is part of Cloud Native PostgreSQL is automatically built via CI/CD pipelines following every commit.
Such images include not only the operator&rsquo;s, but also the operands&rsquo; - specifically every supported PostgreSQL version.
Within the pipelines, images are scanned with:</p>
<ul>
<li><a href="https://github.com/goodwithtech/dockle">Dockle</a>: for best practices in terms
of the container build process</li>
<li><a href="https://github.com/quay/clair">Clair</a>: for vulnerabilities found in both the
underlying operating system as well as libraries and applications that they run</li>
</ul>
<p>!!! Important
All operand images are automatically rebuilt once a day by our pipelines in case
of security updates at the base image and package level, providing <strong>patch level updates</strong>
for the container images that EDB distributes.</p>
<p>The following guidelines and frameworks have been taken into account for container-level security:</p>
<ul>
<li>the <a href="https://dl.dod.cyber.mil/wp-content/uploads/devsecops/pdf/DevSecOps_Enterprise_Container_Image_Creation_and_Deployment_Guide_2.6-Public-Release.pdf">&ldquo;Container Image Creation and Deployment Guide&rdquo;</a>,
developed by the Defense Information Systems Agency (DISA) of the United States Department of Defense (DoD)</li>
<li>the <a href="https://www.cisecurity.org/benchmark/docker/">&ldquo;CIS Benchmark for Docker&rdquo;</a>,
developed by the Center for Internet Security (CIS)</li>
</ul>
<p>!!! Seealso &ldquo;About the Container level security&rdquo;
Please refer to <a href="https://www.enterprisedb.com/blog/security-and-containers-cloud-native-postgresql">&ldquo;Security and Containers in Cloud Native PostgreSQL&rdquo;</a>
blog article for more information about the approach that EDB has taken on
security at the container level in Cloud Native PostgreSQL.</p>
<h2 id="cluster">Cluster</h2>
<p>Security at the cluster level takes into account all Kubernetes components that
form both the control plane and the nodes, as well as the applications that run in
the cluster (PostgreSQL included).</p>
<h3 id="role-based-access-control-rbac">Role Based Access Control (RBAC)</h3>
<p>The operator interacts with the Kubernetes API server with a dedicated service
account called <code>postgresql-operator-manager</code>. In Kubernetes this is installed
by default in the <code>postgresql-operator-system</code> namespace, with a cluster role
binding between this service account and the <code>postgresql-operator-manager</code>
cluster role which defines the set of rules/resources/verbs granted to the operator.
For OpenShift specificities on this matter, please consult the
<a href="openshift.md#predefined-rbac-objects">&ldquo;Red Hat OpenShift&rdquo; section</a>, in particular
<a href="openshift.md#predefined-rbac-objects">&ldquo;Pre-defined RBAC objects&rdquo; section</a>.</p>
<p>!!! Important
The above permissions are exclusively reserved for the operator&rsquo;s service
account to interact with the Kubernetes API server.  They are not directly
accessible by the users of the operator that interact only with <code>Cluster</code>,
<code>Pooler</code>, <code>Backup</code>, and <code>ScheduledBackup</code> resources.</p>
<p>Below we provide some examples and, most importantly, the reasons why Cloud
Native PostgreSQL requires full or partial management of standard Kubernetes
namespaced resources.</p>
<dl>
<dt><code>configmaps</code></dt>
<dd>The operator needs to create and manage default config maps for
the Prometheus exporter monitoring metrics.</dd>
<dt><code>deployments</code></dt>
<dd>The operator needs to manage a PgBouncer connection pooler
using a standard Kubernetes <code>Deployment</code> resource.</dd>
<dt><code>jobs</code></dt>
<dd>The operator needs to handle jobs to manage different <code>Cluster</code>&rsquo;s phases.</dd>
<dt><code>persistentvolumeclaims</code></dt>
<dd>The volume where the <code>PGDATA</code> resides is the
central element of a PostgreSQL <code>Cluster</code> resource; the operator needs
to interact with the selected storage class to dynamically provision
the requested volumes, based on the defined scheduling policies.</dd>
<dt><code>pods</code></dt>
<dd>The operator needs to manage <code>Cluster</code>&rsquo;s instances.</dd>
<dt><code>secrets</code></dt>
<dd>Unless you provide certificates and passwords to your <code>Cluster</code>
objects, the operator adopts the &ldquo;convention over configuration&rdquo; paradigm by
self-provisioning random generated passwords and TLS certificates, and by
storing them in secrets.</dd>
<dt><code>serviceaccounts</code></dt>
<dd>The operator needs to create a service account that
enables the instance manager (which is the <em>PID 1</em> process of the container
that controls the PostgreSQL server) to safely communicate with the
Kubernetes API server to coordinate actions and continuously provide
a reliable status of the <code>Cluster</code>.</dd>
<dt><code>services</code></dt>
<dd>The operator needs to control network access to the PostgreSQL cluster
(or the connection pooler) from applications, and properly manage
failover/switchover operations in an automated way (by assigning, for example,
the correct end-point of a service to the proper primary PostgreSQL instance).</dd>
</dl>
<h3 id="pod-security-policies">Pod Security Policies</h3>
<p>A <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policy</a>
is the Kubernetes way to define security rules and specifications that a pod needs to meet
to run in a cluster.
For InfoSec reasons, every Kubernetes platform should implement them.</p>
<p>Cloud Native PostgreSQL does not require <em>privileged</em> mode for containers execution.
The PostgreSQL containers run as <code>postgres</code> system user. No component whatsoever requires running as <code>root</code>.</p>
<p>Likewise, Volumes access does not require <em>privileges</em> mode or <code>root</code> privileges either.
Proper permissions must be properly assigned by the Kubernetes platform and/or administrators.
The PostgreSQL containers run with a read-only root filesystem (i.e. no writable layer).</p>
<p>The operator explicitly sets the required security contexts.</p>
<p>On Red Hat OpenShift, Cloud Native PostgreSQL runs in <code>restricted</code> security context constraint,
the most restrictive one. The goal is to limit the execution of a pod to a namespace allocated UID
and SELinux context.</p>
<p>!!! Seealso &ldquo;Security Context Constraints in OpenShift&rdquo;
For further information on Security Context Constraints (SCC) in
OpenShift, please refer to the
<a href="https://www.openshift.com/blog/managing-sccs-in-openshift">&ldquo;Managing SCC in OpenShift&rdquo;</a>
article.</p>
<p>!!! Warning &ldquo;Security Context Constraints and namespaces&rdquo;
As stated by <a href="https://docs.openshift.com/container-platform/latest/authentication/managing-security-context-constraints.html#role-based-access-to-ssc_configuring-internal-oauth">Openshift documentation</a>
SCCs are not applied in the default namespaces (<code>default</code>, <code>kube-system</code>,
<code>kube-public</code>, <code>openshift-node</code>, <code>openshift-infra</code>, <code>openshift</code>) and those
should not be used to run pods. CNP clusters deployed in those namespaces
will be unable to start due to missing SCCs.</p>
<h3 id="restricting-pod-access-using-apparmor">Restricting Pod access using AppArmor</h3>
<p>You can assign an
<a href="https://kubernetes.io/docs/tutorials/security/apparmor/">AppArmor</a> profile to
the <code>postgres</code>, <code>initdb</code>, <code>join</code>, <code>full-recovery</code> and <code>bootstrap-controller</code> containers inside every <code>Cluster</code> pod through the
<code>container.apparmor.security.beta.kubernetes.io</code> annotation.</p>
<p>!!! Seealso &ldquo;Example of cluster annotations&rdquo;</p>
<pre tabindex="0"><code>	kind: Cluster
	metadata:
		name: cluster-apparmor
		annotations:
			container.apparmor.security.beta.kubernetes.io/postgres: runtime/default
			container.apparmor.security.beta.kubernetes.io/initdb: runtime/default
			container.apparmor.security.beta.kubernetes.io/join: runtime/default
</code></pre><p>!!! Warning
Using this kind of annotations can result in your cluster to stop working.
If this is the case, the annotation can be safely removed from the <code>Cluster</code>.</p>
<p>The AppArmor configuration must be at Kubernetes node level, meaning that the
underlying operating system must have this option enable and properly
configured.</p>
<p>In case this is not the situation, and the annotations were added at the
<code>Cluster</code> creation time, pods will not be created. On the other hand, if you
add the annotations after the <code>Cluster</code> was created the pods in the cluster will
be unable to start and you will get an error like this:</p>
<pre tabindex="0"><code>metadata.annotations[container.apparmor.security.beta.kubernetes.io/postgres]: Forbidden: may not add AppArmor annotations]
</code></pre><p>In such cases, please refer to your Kubernetes administrators and ask for the proper AppArmor profile to use.</p>
<p>!!! Warning &ldquo;AppArmor and OpenShift&rdquo;
AppArmor is currently available only on Debian distributions like Ubuntu,
hence this is not (and will not be) available in OpenShift</p>
<h3 id="network-policies">Network Policies</h3>
<p>The pods created by the <code>Cluster</code> resource can be controlled by Kubernetes
<a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies</a>
to enable/disable inbound and outbound network access at IP and TCP level.</p>
<p>!!! Important
The operator needs to communicate to each instance on TCP port 8000
to get information about the status of the PostgreSQL server. Please
make sure you keep this in mind in case you add any network policy,
and refer to the &ldquo;Exposed Ports&rdquo; section below for a list of ports used by
Cloud Native PostgreSQL for finer control.</p>
<p>Network policies are beyond the scope of this document.
Please refer to the <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">&ldquo;Network policies&rdquo;</a>
section of the Kubernetes documentation for further information.</p>
<h4 id="exposed-ports">Exposed Ports</h4>
<p>Cloud Native PostgreSQL exposes ports at operator, instance manager and operand
levels, as listed in the table below:</p>
<table>
<thead>
<tr>
<th style="text-align:left">System</th>
<th style="text-align:left">Port number</th>
<th style="text-align:left">Exposing</th>
<th style="text-align:left">Name</th>
<th style="text-align:left">Certificates</th>
<th style="text-align:left">Authentication</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">operator</td>
<td style="text-align:left">9443</td>
<td style="text-align:left">webhook server</td>
<td style="text-align:left"><code>webhook-server</code></td>
<td style="text-align:left">TLS</td>
<td style="text-align:left">Yes</td>
</tr>
<tr>
<td style="text-align:left">operator</td>
<td style="text-align:left">8080</td>
<td style="text-align:left">metrics</td>
<td style="text-align:left"><code>metrics</code></td>
<td style="text-align:left">no TLS</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">instance manager</td>
<td style="text-align:left">9187</td>
<td style="text-align:left">metrics</td>
<td style="text-align:left"><code>metrics</code></td>
<td style="text-align:left">no TLS</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">instance manager</td>
<td style="text-align:left">8000</td>
<td style="text-align:left">status</td>
<td style="text-align:left"><code>status</code></td>
<td style="text-align:left">no TLS</td>
<td style="text-align:left">No</td>
</tr>
<tr>
<td style="text-align:left">operand</td>
<td style="text-align:left">5432</td>
<td style="text-align:left">PostgreSQL instance</td>
<td style="text-align:left"><code>postgresql</code></td>
<td style="text-align:left">optional TLS</td>
<td style="text-align:left">Yes</td>
</tr>
</tbody>
</table>
<h3 id="postgresql">PostgreSQL</h3>
<p>The current implementation of Cloud Native PostgreSQL automatically creates
passwords and <code>.pgpass</code> files for the <code>postgres</code> superuser and the database owner.</p>
<p>As far as encryption of password is concerned, Cloud Native PostgreSQL follows
the default behavior of PostgreSQL: starting from PostgreSQL 14,
<code>password_encryption</code> is by default set to <code>scram-sha-256</code>, while on earlier
versions it is set to <code>md5</code>.</p>
<p>!!! Important
Please refer to the <a href="https://www.postgresql.org/docs/current/auth-password.html">&ldquo;Password authentication&rdquo;</a>
section in the PostgreSQL documentation for details.</p>
<p>You can disable management of the <code>postgres</code> user password via secrets by setting
<code>enableSuperuserAccess</code> to <code>false</code>.</p>
<p>!!! Note
The operator supports toggling the <code>enableSuperuserAccess</code> option. When you
disable it on a running cluster, the operator will ignore the content of the secret,
remove it (if previously generated by the operator) and set the password of the
<code>postgres</code> user to <code>NULL</code> (de facto disabling remote access through password authentication).</p>
<p>See the <a href="architecture.md#secrets">&ldquo;Secrets&rdquo; section in the &ldquo;Architecture&rdquo; page</a> for more information.</p>
<p>You can use those files to configure application access to the database.</p>
<p>By default, every replica is automatically configured to connect in <strong>physical
async streaming replication</strong> with the current primary instance, with a special
user called <code>streaming_replica</code>. The connection between nodes is <strong>encrypted</strong>
and authentication is via <strong>TLS client certificates</strong> (please refer to the
[&ldquo;Client TLS/SSL Connections&rdquo;](ssl_connections.md#&ldquo;Client TLS/SSL Connections&rdquo;) page
for details).</p>
<p>Currently, the operator allows administrators to add <code>pg_hba.conf</code> lines directly in the manifest
as part of the <code>pg_hba</code> section of the <code>postgresql</code> configuration. The lines defined in the
manifest are added to a default <code>pg_hba.conf</code>.</p>
<p>For further detail on how <code>pg_hba.conf</code> is managed by the operator, see the
<a href="postgresql_conf.md#the-pg_hba-section">&ldquo;PostgreSQL Configuration&rdquo; page</a> of the documentation.</p>
<p>!!! Important
Examples assume that the Kubernetes cluster runs in a private and secure network.</p>



    <footer class="text-center p-24">
    <div>
        <a href="#" class="bg-rose-600 py-4 px-12 rounded-full text-white text-xl">View on GitHub</a>
    </div>
    <p class="text-sm text-slate-600 w-1/2 mx-auto pt-10">&copy; 2019-2022 The CloudNativePG Contributors<br/>
    The Linux Foundation has registered trademarks and uses trademarks. 
    For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage/" class="text-rose-600">Trademark Usage page</a>.<br/>
    Postgres, PostgreSQL and the Slonik Logo are trademarks or registered trademarks of the PostgreSQL Community Association of Canada, and used with their permission.
    </p>
</footer>

  </div>
  
  <script src="https://cdn.usefathom.com/script.js" data-site="GSMQCVAJ" defer></script>
  
</body>

</html>
<!DOCTYPE html>




<html lang="en">

<head>
  <title>CloudNative PG</title>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <meta name="description" content="CloudNativePG is the Kubernetes operator that covers the full lifecycle of a highly available PostgreSQL database cluster with a primary/standby architecture, using native streaming replication." />
  <meta name="keywords" content="" />
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet" />
  <link href="/main.min.css" rel="stylesheet" />
  <link rel="favicon" href="/favicon/favicon.ico">
</head>

<body>
  <div class="h-screen w-screen">
    
<main>
  <header>
    <div class="flex flex-row px-16 justify-between py-5">
      <div>
        
        <a href="/"><img src="/logo/large_logo.svg" class="h-14" alt="Cloud Native Postgres Logo"></a>
      </div>
      <div>
        <ul class="flex gap-14 text-2xl text-slate-400 my-5">
          <li><a href="/docs/1.15.0/" target="_blank">Documentation</a></li>
          <li>Support</li>
          <li>End users</li>
          <li>Blog</li>
        </ul>
      </div>
      <div class="flex gap-5 my-4">
        <a href="https://github.com/cloudnative-pg/cloudnative-pg">
          <img src="/icons/Git.svg" alt="Github">
        </a>
        <a href="https://cloudnativepg.slack.com/">
          <img src="/icons/Slack.svg" alt="Slack">
        </a>
        <a href="https://twitter.com/CloudNativePg">
          <img src="/icons/Twitter.svg" alt="Twitter">
        </a>
        <a href="https://www.youtube.com/channel/UCTGH88W1BiuRRPTzJUDPJyA">
          <img src="/icons/YouTube.svg" alt="YouTube">
        </a>
      </div>
    </div>
  </header>
</main>


    
<h2 class="text-2xl"></h2>
<h1 id="storage">Storage</h1>
<p><strong>Storage is the most critical component in a database workload</strong>.
Storage should be always available, scale, perform well,
and guarantee consistency and durability. The same expectations and
requirements that apply to traditional environments, such as virtual machines
and bare metal, are also valid in container contexts managed by Kubernetes.</p>
<p>!!! Important
Kubernetes has its own specificities, when it comes to dynamically
provisioned storage. These include <em>storage classes</em>, <em>persistent
volumes</em>, and <em>persistent volume claims</em>. You need to own these
concepts, on top of all the valuable knowledge you have built over
the years in terms of storage for database workloads on VMs and
physical servers.</p>
<p>There are two primary methods of access to storage:</p>
<ul>
<li><strong>network</strong>: either directly or indirectly (think of an NFS volume locally mounted on a host running Kubernetes)</li>
<li><strong>local</strong>: directly attached to the node where a Pod is running (this also includes directly attached disks on bare metal installations of Kubernetes)</li>
</ul>
<p>Network storage, which is the most common usage pattern in Kubernetes,
presents the same issues of throughput and latency that you can
experience in a traditional environment. These can be accentuated in
a shared environment, where I/O contention with several applications
increases the variability of performance results.</p>
<p>Local storage enables shared-nothing architectures, which is more suitable
for high transactional and Very Large DataBase (VLDB) workloads, as it
guarantees higher and more predictable performance.</p>
<p>!!! Warning
Before you deploy a PostgreSQL cluster with Cloud Native PostgreSQL,
ensure that the storage you are using is recommended for database
workloads. Our advice is to clearly set performance expectations by
first benchmarking the storage using tools such as <a href="https://fio.readthedocs.io/en/latest/fio_doc.html">fio</a>,
and then the database using <a href="https://www.postgresql.org/docs/current/pgbench.html">pgbench</a>.</p>
<h2 id="benchmarking-cloud-native-postgresql">Benchmarking Cloud Native PostgreSQL</h2>
<p>EDB maintains <a href="https://github.com/EnterpriseDB/cnp-bench">cnp-bench</a>,
an open source set of guidelines and Helm charts for benchmarking Cloud Native PostgreSQL
in a controlled Kubernetes environment, before deploying the database in production.</p>
<p>Briefly, <code>cnp-bench</code> is designed to operate at two levels:</p>
<ul>
<li>measuring the performance of the underlying storage using <code>fio</code>, with relevant
metrics for database workloads such as throughput for sequential reads, sequential
writes, random reads and random writes</li>
<li>measuring the performance of the database using the default benchmarking tool
distributed along with PostgreSQL: <code>pgbench</code></li>
</ul>
<p>!!! Important
Measuring both the storage and database performance is an activity that
must be done <strong>before the database goes in production</strong>. However, such results
are extremely valuable not only in the planning phase (e.g., capacity planning),
but also in the production lifecycle, especially in emergency situations
(when we don&rsquo;t have the luxury anymore to run this kind of tests). Databases indeed
change and evolve over time, so does the distribution of data, potentially affecting
performance: knowing the theoretical maximum throughput of sequential reads or
writes will turn out to be extremely useful in those situations. Especially in
shared-nothing contexts, where results do not vary due to the influence of external workloads.
<strong>Know your system, benchmark it.</strong></p>
<h2 id="persistent-volume-claim">Persistent Volume Claim</h2>
<p>The operator creates a persistent volume claim (PVC) for each PostgreSQL
instance, with the goal to store the <code>PGDATA</code>, and then mounts it into each Pod.</p>
<h2 id="configuration-via-a-storage-class">Configuration via a storage class</h2>
<p>The easier way to configure the storage for a PostgreSQL class is to just
request storage of a certain size, like in the following example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">postgresql.k8s.enterprisedb.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Cluster</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">postgresql-storage-class</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">instances</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storage</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">size</span>: <span style="color:#ae81ff">1Gi</span>
</span></span></code></pre></div><p>Using the previous configuration, the generated PVCs will be satisfied by the default storage
class. If the target Kubernetes cluster has no default storage class, or even if you need your PVCs
to be satisfied by a known storage class, you can set it into the custom resource:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">postgresql.k8s.enterprisedb.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Cluster</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">postgresql-storage-class</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">instances</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storage</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">storageClass</span>: <span style="color:#ae81ff">standard</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">size</span>: <span style="color:#ae81ff">1Gi</span>
</span></span></code></pre></div><p>!!! Important
Cloud Native PostgreSQL has been designed to be storage class agnostic.
As usual, our recommendation is to properly benchmark the storage class
in a controlled environment, before hitting production.</p>
<h2 id="configuration-via-a-pvc-template">Configuration via a PVC template</h2>
<p>To further customize the generated PVCs, you can provide a PVC template inside the Custom Resource,
like in the following example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">postgresql.k8s.enterprisedb.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Cluster</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">postgresql-pvc-template</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">instances</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storage</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">pvcTemplate</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">1Gi</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">storageClassName</span>: <span style="color:#ae81ff">standard</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumeMode</span>: <span style="color:#ae81ff">Filesystem</span>
</span></span></code></pre></div><h2 id="volume-expansion">Volume expansion</h2>
<p>Kubernetes exposes an API allowing <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">expanding PVCs</a>
that is enabled by default but needs to be supported by the underlying <code>StorageClass</code>.</p>
<p>To check if a certain <code>StorageClass</code> supports volume expansion, you can read the <code>allowVolumeExpansion</code>
field for your storage class:</p>
<pre tabindex="0"><code>$ kubectl get storageclass -o jsonpath=&#39;{$.allowVolumeExpansion}&#39; premium-storage
true
</code></pre><h3 id="using-the-volume-expansion-kubernetes-feature">Using the volume expansion Kubernetes feature</h3>
<p>Given the storage class supports volume expansion, you can change the size requirement
of the <code>Cluster</code>, and the operator will apply the change to every PVC.</p>
<p>If the <code>StorageClass</code> supports <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#resizing-an-in-use-persistentvolumeclaim">online volume resizing</a>
the change is immediately applied to the Pods. If the underlying Storage Class doesn&rsquo;t support
that, you will need to delete the Pod to trigger the resize.</p>
<p>The best way to proceed is to delete one Pod at a time, starting from replicas and waiting
for each Pod to be back up.</p>
<h3 id="expanding-pvc-volumes-on-aks">Expanding PVC volumes on AKS</h3>
<p>At the moment, <a href="https://github.com/Azure/AKS/issues/1477">Azure is not able to resize the PVC&rsquo;s volume without restarting the pod</a>.
Cloud Native PostgreSQL has overcome this limitation through the
<code>ENABLE_AZURE_PVC_UPDATES</code> environment variable in the
<a href="operator_conf.md#available-options">operator configuration</a>.
When set to <code>'true'</code>, Cloud Native PostgreSQL triggers a rolling update of the
Postgres cluster.</p>
<p>Alternatively, you can follow the workaround below to manually resize the
volume in AKS.</p>
<h4 id="workaround-for-volume-expansion-on-aks">Workaround for volume expansion on AKS</h4>
<p>You can manually resize a PVC on AKS by following these procedures.
As an example, let&rsquo;s suppose you have a cluster with 3 replicas:</p>
<pre tabindex="0"><code>$ kubectl get pods
NAME                READY   STATUS    RESTARTS   AGE
cluster-example-1   1/1     Running   0          2m37s
cluster-example-2   1/1     Running   0          2m22s
cluster-example-3   1/1     Running   0          2m10s
</code></pre><p>An Azure disk can only be expanded while in &ldquo;unattached&rdquo; state, as described in the
<a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver/blob/master/docs/known-issues/sizegrow.md">docs</a>.  <!-- raw HTML omitted -->
This means, that to resize a disk used by a PostgreSQL cluster, you will need to perform a manual rollout,
first cordoning the node that hosts the Pod using the PVC bound to the disk. This will prevent the Operator
to recreate the Pod and immediately reattach it to its PVC before the background disk resizing has been completed.</p>
<p>First step is to edit the cluster definition applying the new size, let&rsquo;s say &ldquo;2Gi&rdquo;, as follows:</p>
<pre tabindex="0"><code>apiVersion: postgresql.k8s.enterprisedb.io/v1
kind: Cluster
metadata:
  name: cluster-example
spec:
  instances: 3

  storage:
    storageClass: default
    size: 2Gi
</code></pre><p>Assuming the <code>cluster-example-1</code> Pod is the cluster&rsquo;s primary, we can proceed with the replicas first.
For example start with cordoning the kubernetes node that hosts the <code>cluster-example-3</code> Pod:</p>
<pre tabindex="0"><code>kubectl cordon &lt;node of cluster-example-3&gt;
</code></pre><p>Then delete the <code>cluster-example-3</code> Pod:</p>
<pre tabindex="0"><code>$ kubectl delete pod/cluster-example-3
</code></pre><p>Run the following command:</p>
<pre tabindex="0"><code>kubectl get pvc -w -o=jsonpath=&#39;{.status.conditions[].message}&#39; cluster-example-3
</code></pre><p>Wait until you see the following output:</p>
<pre tabindex="0"><code>Waiting for user to (re-)start a Pod to finish file system resize of volume on node.
</code></pre><p>Then, you can uncordon the node:</p>
<pre tabindex="0"><code>kubectl uncordon &lt;node of cluster-example-3&gt;
</code></pre><p>Wait for the Pod to be recreated correctly and get in Running and Ready state:</p>
<pre tabindex="0"><code>kubectl get pods -w cluster-example-3
cluster-example-3   0/1     Init:0/1   0          12m
cluster-example-3   1/1     Running   0          12m
</code></pre><p>Now verify the PVC expansion by running the following command, which should return &ldquo;2Gi&rdquo; as configured:</p>
<pre tabindex="0"><code>kubectl get pvc cluster-example-3 -o=jsonpath=&#39;{.status.capacity.storage}&#39;
</code></pre><p>So, you can repeat these steps for the remaining Pods.</p>
<p>!!! Important
Please leave the resizing of the disk associated with the primary instance as last disk,
after promoting through a switchover a new resized Pod, using <code>kubectl cnp promote</code>
(e.g. <code>kubectl cnp promote cluster-example 3</code> to promote <code>cluster-example-3</code> to primary).</p>
<h3 id="recreating-storage">Recreating storage</h3>
<p>If the storage class does not support volume expansion, you can still regenerate your cluster
on different PVCs, by allocating new PVCs with increased storage and then move the
database there. This operation is feasible only when the cluster contains more than one node.</p>
<p>While you do that, you need to prevent the operator from changing the existing PVC
by disabling the <code>resizeInUseVolumes</code> flag, like in the following example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">postgresql.k8s.enterprisedb.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Cluster</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">postgresql-pvc-template</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">instances</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storage</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">storageClass</span>: <span style="color:#ae81ff">standard</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">size</span>: <span style="color:#ae81ff">1Gi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resizeInUseVolumes</span>: <span style="color:#66d9ef">False</span>
</span></span></code></pre></div><p>In order to move the entire cluster to a different storage area, you need to recreate all the PVCs and
all the Pods. Let&rsquo;s suppose you have a cluster with three replicas like in the following
example:</p>
<pre tabindex="0"><code>$ kubectl get pods
NAME                READY   STATUS    RESTARTS   AGE
cluster-example-1   1/1     Running   0          2m37s
cluster-example-2   1/1     Running   0          2m22s
cluster-example-3   1/1     Running   0          2m10s
</code></pre><p>To recreate the cluster using different PVCs, you can edit the cluster definition to disable
<code>resizeInUseVolumes</code>, and then recreate every instance in a different PVC.</p>
<p>As an example, to recreate the storage for <code>cluster-example-3</code> you can:</p>
<pre tabindex="0"><code>$ kubectl delete pvc/cluster-example-3 pod/cluster-example-3
</code></pre><p>Having done that, the operator will orchestrate the creation of another replica with a
resized PVC:</p>
<pre tabindex="0"><code>$ kubectl get pods
NAME                           READY   STATUS      RESTARTS   AGE
cluster-example-1              1/1     Running     0          5m58s
cluster-example-2              1/1     Running     0          5m43s
cluster-example-4-join-v2bfg   0/1     Completed   0          17s
cluster-example-4              1/1     Running     0          10s
</code></pre>


    <footer class="text-center p-24">
    <div>
        <a href="#" class="bg-rose-600 py-4 px-12 rounded-full text-white text-xl">View on GitHub</a>
    </div>
    <p class="text-sm text-slate-600 w-1/2 mx-auto pt-10">&copy; 2019-2022 The CloudNativePG Contributors<br/>
    The Linux Foundation has registered trademarks and uses trademarks. 
    For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage/" class="text-rose-600">Trademark Usage page</a>.<br/>
    Postgres, PostgreSQL and the Slonik Logo are trademarks or registered trademarks of the PostgreSQL Community Association of Canada, and used with their permission.
    </p>
</footer>

  </div>
  
  <script src="https://cdn.usefathom.com/script.js" data-site="GSMQCVAJ" defer></script>
  
</body>

</html>
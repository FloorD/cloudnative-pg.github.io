<!DOCTYPE html>




<html lang="en">

<head>
  <title>CloudNative PG</title>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <meta name="description" content="CloudNativePG is the Kubernetes operator that covers the full lifecycle of a highly available PostgreSQL database cluster with a primary/standby architecture, using native streaming replication." />
  <meta name="keywords" content="" />
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet" />
  <link href="/main.min.css" rel="stylesheet" />
  <link rel="favicon" href="/favicon/favicon.ico">
</head>

<body>
  <div class="h-screen w-screen">
    
<main>
  <header>
    <div class="flex flex-row px-16 justify-between py-5">
      <div>
        
        <a href="/"><img src="/logo/large_logo.svg" class="h-14" alt="Cloud Native Postgres Logo"></a>
      </div>
      <div>
        <ul class="flex gap-14 text-2xl text-slate-400 my-5">
          <li><a href="/docs/1.15.0/" target="_blank">Documentation</a></li>
          <li>Support</li>
          <li>End users</li>
          <li>Blog</li>
        </ul>
      </div>
      <div class="flex gap-5 my-4">
        <a href="https://github.com/cloudnative-pg/cloudnative-pg">
          <img src="/icons/Git.svg" alt="Github">
        </a>
        <a href="https://cloudnativepg.slack.com/">
          <img src="/icons/Slack.svg" alt="Slack">
        </a>
        <a href="https://twitter.com/CloudNativePg">
          <img src="/icons/Twitter.svg" alt="Twitter">
        </a>
        <a href="https://www.youtube.com/channel/UCTGH88W1BiuRRPTzJUDPJyA">
          <img src="/icons/YouTube.svg" alt="YouTube">
        </a>
      </div>
    </div>
  </header>
</main>


    
<h2 class="text-2xl"></h2>
<h1 id="replication">Replication</h1>
<p>Physical replication is one of the strengths of PostgreSQL and one of the
reasons why some of the world&rsquo;s largest organizations in the world have chosen
it for the management of their data in business continuity contexts.
Primarily used to achieve high availability, physical replication also allows
scale-out of read-only workloads and offloading some work from the primary.</p>
<h2 id="application-level-replication">Application-level replication</h2>
<p>Having contributed throughout the years to the replication feature in PostgreSQL,
we have decided to build high availability in Cloud Native PostgreSQL on top of
the native physical replication technology and integrate it
directly in the Kubernetes API.</p>
<p>In Kubernetes terms, this is referred to as <strong>application-level replication</strong>, in
contrast with <em>storage-level replication</em>.</p>
<h2 id="a-very-mature-technology">A very mature technology</h2>
<p>PostgreSQL has a very robust and mature native framework for replicating data
from the primary instance to one or more replicas, built around the
concept of transactional changes continuously stored in the WAL (Write Ahead Log).</p>
<p>Started as the evolution of crash recovery and point in time recovery
technologies, physical replication was first introduced in PostgreSQL 8.2
(2006) through WAL shipping from the primary to a warm standby in
continuous recovery.</p>
<p>PostgreSQL 9.0 (2010) enhanced it with WAL streaming and read-only replicas via
<em>hot standby</em>, while 9.1 (2011) introduced synchronous replication at the
transaction level (for RPO=0 clusters). Cascading replication was released with
PostgreSQL 9.2 (2012). The foundations of logical replication were laid in
PostgreSQL 9.4, while version 10 (2017) introduced native support for the
publisher/subscriber pattern to replicate data from an origin to a destination.</p>
<h2 id="replication-within-a-postgresql-cluster">Replication within a PostgreSQL cluster</h2>
<h3 id="streaming-replication-support">Streaming replication support</h3>
<p>At the moment, Cloud Native PostgreSQL natively and transparently manages
physical streaming replicas within a cluster in a declarative way, based on
the number of provided <code>instances</code> in the <code>spec</code>:</p>
<pre tabindex="0"><code>replicas = instances - 1 (where  instances &gt; 0)
</code></pre><p>Immediately after the initialization of a cluster, the operator creates a user
called <code>streaming_replica</code> as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">USER</span> streaming_replica <span style="color:#66d9ef">WITH</span> REPLICATION;
</span></span><span style="display:flex;"><span>   <span style="color:#75715e">-- NOSUPERUSER INHERIT NOCREATEROLE NOCREATEDB NOBYPASSRLS
</span></span></span></code></pre></div><p>!!! Note
Due to a <code>pg_rewind</code> requirement, in PostgreSQL 10 the <code>streaming_replica</code>
user is created with <code>SUPERUSER</code> privileges.</p>
<p>Out of the box, the operator automatically sets up streaming replication within
the cluster over an encrypted channel and enforces TLS client certificate
authentication for the <code>streaming_replica</code> user - as highlighted by the following
excerpt taken from <code>pg_hba.conf</code>:</p>
<pre tabindex="0"><code># Require client certificate authentication for the streaming_replica user
hostssl postgres streaming_replica all cert
hostssl replication streaming_replica all cert
</code></pre><p>!!! Seealso &ldquo;Certificates&rdquo;
For details on how Cloud Native PostgreSQL manages certificates, please refer
to the <a href="certificates.md#client-streaming_replica-certificate">&ldquo;Certificates&rdquo; section</a>
in the documentation.</p>
<h3 id="continuous-backup-integration">Continuous backup integration</h3>
<p>In case continuous backup is configured in the cluster, Cloud Native PostgreSQL
transparently configures replicas to take advantage of <code>restore_command</code> when
in continuous recovery. As a result, PostgreSQL is able to use the WAL archive
as a fallback option everytime pulling WALs via streaming replication fails.</p>
<h3 id="synchronous-replication">Synchronous replication</h3>
<p>Cloud Native PostgreSQL supports configuration of <strong>quorum-based synchronous
streaming replication</strong> via two configuration options called <code>minSyncReplicas</code>
and <code>maxSyncReplicas</code> which are the minimum and maximum number of expected
synchronous standby replicas available at any time.
For self-healing purposes, the operator always weights these two values with
the available number of replicas in order to determine the quorum.</p>
<p>Synchronous replication is disabled by default (<code>minSyncReplicas</code> and
<code>maxSyncReplicas</code> are not defined).
In case both <code>minSyncReplicas</code> and <code>maxSyncReplicas</code> are set, Cloud Native
PostgreSQL automatically updates the <code>synchronous_standby_names</code> option in
PostgreSQL to the following value:</p>
<pre tabindex="0"><code>ANY q (pod1, pod2, ...)
</code></pre><p>Where:</p>
<ul>
<li><code>q</code> is an integer automatically calculated by the operator to be:<br>
<code>1 &lt;= minSyncReplicas &lt;= q &lt;= maxSyncReplicas &lt;= readyReplicas</code></li>
<li><code>pod1, pod2, ...</code> is the list of all PostgreSQL pods in the cluster</li>
</ul>
<p>!!! Warning
To provide self-healing capabilities, the operator has the power
to ignore <code>minSyncReplicas</code> in case such value is higher than the currently
available number of replicas. Synchronous replication is automatically disabled
when <code>readyReplicas</code> is <code>0</code>.</p>
<p>As stated in the
<a href="https://www.postgresql.org/docs/current/warm-standby.html#SYNCHRONOUS-REPLICATION">PostgreSQL documentation</a>,
the <em>method <code>ANY</code> specifies a quorum-based synchronous replication and makes
transaction commits wait until their WAL records are replicated to at least the
requested number of synchronous standbys in the list</em>.</p>
<p>!!! Important
Even though the operator privileges self-healing over enforcement of
synchronous replication settings, our recommendation is to plan for
synchronous replication only in clusters with 3+ instances or,
more generally, when <code>maxSyncReplicas &lt; (instances - 1)</code>.</p>
<h2 id="replication-from-an-external-postgresql-cluster">Replication from an external PostgreSQL cluster</h2>
<p>Cloud Native PostgreSQL relies on the foundations of the PostgreSQL replication
framework even when a PostgreSQL cluster is created from an existing one (source)
and kept synchronized through the
<a href="architecture.md#multi-cluster-deployments">replica cluster</a> feature. The source
can be a primary cluster or another replica cluster (cascading replica cluster).</p>
<p>The available options in terms of replication, both at bootstrap and continuous
recovery level, are:</p>
<ul>
<li>use streaming replication between the replica cluster and the source
(this will certainly require some administrative and security related
work to be done to make sure that the network connection between the
two clusters is correctly setup)</li>
<li>use a Barman Cloud object store for recovery of the base backups and
the WAL files that are regularly shipped from the source to the object
store and pulled by <code>barman-cloud-wal-restore</code> in the replica cluster</li>
<li>any of the two</li>
</ul>
<p>All you have to do is actually define an external cluster.
Please refer to the <a href="bootstrap.md#bootstrap-from-another-cluster">&ldquo;Bootstrap&rdquo; section</a>
for information on how to clone a PostgreSQL server using either
<code>pg_basebackup</code> (streaming) or <code>recovery</code> (object store).</p>
<p>If the external cluster contains a <code>barmanObjectStore</code> section:</p>
<ul>
<li>you&rsquo;ll be able to boostrap the replica cluster from an object store
using the <code>recovery</code> section</li>
<li>Cloud Native PostgreSQL will automatically set the <code>restore_command</code>
in the designated primary instance</li>
</ul>
<p>If the external cluster contains a <code>connectionParameters</code> section:</p>
<ul>
<li>you&rsquo;ll be able to boostrap the replica cluster via streaming replication
using the <code>pg_basebackup</code> section</li>
<li>Cloud Native PostgreSQL will automatically set the <code>primary_conninfo</code>
option in the designated primary instance, so that a WAL receiver
process is started to connect to the source cluster and receive data</li>
</ul>
<p>The created replica cluster can perform backups in a reserved object store from
the designated primary, enabling symmetric architectures in a distributed
fashion.</p>
<p>You have full flexibility and freedom to decide your favourite
distributed architecture for a PostgreSQL database, by choosing:</p>
<ul>
<li>a private cloud spanning over multiple Kubernetes clusters in different data
centers</li>
<li>a public cloud spanning over multiple Kubernetes clusters in different
regions</li>
<li>a mix of the previous two (hybrid)</li>
<li>a public cloud spanning over multiple Kubernetes clusters in different
regions and on different Cloud Service Providers</li>
</ul>



    <footer class="text-center p-24">
    <div>
        <a href="#" class="bg-rose-600 py-4 px-12 rounded-full text-white text-xl">View on GitHub</a>
    </div>
    <p class="text-sm text-slate-600 w-1/2 mx-auto pt-10">&copy; 2019-2022 The CloudNativePG Contributors<br/>
    The Linux Foundation has registered trademarks and uses trademarks. 
    For a list of trademarks of The Linux Foundation, please see our <a href="https://www.linuxfoundation.org/trademark-usage/" class="text-rose-600">Trademark Usage page</a>.<br/>
    Postgres, PostgreSQL and the Slonik Logo are trademarks or registered trademarks of the PostgreSQL Community Association of Canada, and used with their permission.
    </p>
</footer>

  </div>
  
  <script src="https://cdn.usefathom.com/script.js" data-site="GSMQCVAJ" defer></script>
  
</body>

</html>
<!DOCTYPE html>




<html lang="en">

<head>
  <title>CloudNative PG</title>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <meta name="description" content="CloudNativePG is the Kubernetes operator that covers the full lifecycle of a highly available PostgreSQL database cluster with a primary/standby architecture, using native streaming replication." />
  <meta name="keywords" content="" />
  <link rel="preload" href="/css/Supreme-Variable.woff2" as="font" type="font/woff2" crossorigin="anonymous" />
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet" />
  <link href="/main.min.css" rel="stylesheet" />
  <link rel="favicon" href="/favicon/favicon.ico">
  <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

<body>
  <div class="h-screen w-screen">
    
<main>
  <header>
    <div class="md:flex md:flex-row md:px-16 md:justify-between py-5">
      <div class="flex justify-between px-4 md:hidden">
        <div>
          
          <a href="/"><img src="/logo/large_logo.svg" class="md:h-14 h-9" alt="Cloud Native Postgres Logo"></a>
        </div>
        <div class="h-6 my-auto" id="toggle-button">
          <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
            <path stroke-linecap="round" stroke-linejoin="round" d="M4 6h16M4 12h16M4 18h16" />
          </svg>
        </div>
      </div>
      <div class="md:block hidden">
        
        <a href="/"><img src="/logo/large_logo.svg" class="h-14" alt="Cloud Native Postgres Logo"></a>
      </div>
      
      <div class="hidden md:block">
        <ul class="md:flex gap-14 text-2xl medium-gray my-5">
          <li><a href="/docs">Documentation</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="/support">Support</a></li>
          <li><a href="/end_users">End Users</a></li>
        </ul>
      </div>
      
      <div class="py-4 bg-gray-1 mt-5 hidden" id="navi-list">
        <ul class="md:flex gap-14 text-2xl md:medium-gray charcoal">
          <li class="text-center py-4"><a href="/docs">Documentation</a></li>
          <li class="text-center py-4"><a href="/blog">Blog</a></li>
          <li class="text-center py-4"><a href="/support">Support</a></li>
          <li class="text-center py-4"><a href="/end_users">End Users</a></li>
        </ul>
        <div class="flex gap-3 py-4 justify-center bg-gray-1 fill-cnp-blue">
          <a href="https://github.com/cloudnative-pg/cloudnative-pg">
            <img src="/icons/Git.svg" alt="Github">
          </a>
          <a href="https://cloudnativepg.slack.com/">
            <img src="/icons/Slack.svg" alt="Slack">
          </a>
          <a href="https://twitter.com/CloudNativePg">
            <img src="/icons/Twitter.svg" alt="Twitter">
          </a>
          <a href="https://www.youtube.com/channel/UCTGH88W1BiuRRPTzJUDPJyA">
            <img src="/icons/YouTube.svg" alt="YouTube">
          </a>
        </div>
      </div>
      

      <div class="flex gap-5 my-4 fill-cnp-blue hidden md:flex">
       <a class="github-button" href="https://github.com/cloudnative-pg/cloudnative-pg"
            data-color-scheme="no-preference: light; light: light; dark: dark;"
            data-size="large" data-show-count="true"
            aria-label="Star cloudnative-pg/cloudnative-pg on GitHub">Star</a>
        <a href="https://join.slack.com/t/cloudnativepg/shared_invite/zt-17culux7k-P_UsVOOR9teUYi4dGhDSBQ">
          <img src="/icons/Slack.svg" alt="Slack" title="Join our Slack channel now!">
        </a>
        <a href="https://twitter.com/CloudNativePg">
          <img src="/icons/Twitter.svg" alt="Twitter">
        </a>
        <a href="https://www.youtube.com/channel/UCTGH88W1BiuRRPTzJUDPJyA">
          <img src="/icons/YouTube.svg" alt="YouTube">
        </a>
      </div>
    </div>
  </header>
</main>
<script>
  const toggleButton = document.getElementById('toggle-button')
  const naviList = document.getElementById('navi-list')

  toggleButton.addEventListener('click', () => {
    naviList.classList.toggle('hidden');
  })
</script>


    
<base href="..">
<div class="md:px-56 md:pt-20 md:flex">
    <div class="w-1/4 px-4 bg-purple-2 pt-14 leading-9 md:block hidden">
        
<ul>
    
    
    
    
    <li>
        <a href="/docs/v1.15.0/api_reference/">API Reference</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/applications/">Connecting from an application</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/architecture/">Architecture</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/backup_recovery/">Backup and Recovery</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/before_you_start/">Before You Start</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/bootstrap/">Bootstrap</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/certificates/">Certificates</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/cnpg-plugin/">CloudNativePG Plugin</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/commercial_support/">Commercial support</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/connection_pooling/">Connection Pooling</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/container_images/">Container Image Requirements</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/e2e/">End-to-End Tests</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/expose_pg_services/">Exposing Postgres Services</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/failover/">Automated failover</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/failure_modes/">Failure Modes</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/fencing/">Fencing</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/installation_upgrade/">Installation and upgrades</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/instance_manager/">Postgres instance manager</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/kubernetes_upgrade/">Kubernetes Upgrade</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/labels_annotations/">Labels and annotations</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/logging/">Logging</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/monitoring/">Monitoring</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/operator_capability_levels/">Operator Capability Levels</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/operator_conf/">Operator configuration</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/postgresql_conf/">PostgreSQL Configuration</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/quickstart/">Quickstart</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/release_notes/">Release notes</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/replication/">Replication</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/resource_management/">Resource management</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/rolling_update/">Rolling Updates</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/samples/">Configuration Samples</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/scheduling/">Scheduling</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/security/">Security</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/ssl_connections/">Client TLS/SSL Connections</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/storage/">Storage</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/supported_releases/">Supported releases</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/troubleshooting/">Troubleshooting</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/use_cases/">Use cases</a>
    </li>
    
    
    
</ul>

<hr class="w-1/2">


<p>Documention by version</p>
<ul>
    
    <li>
        <a href="/docs/v1.15.0/">
            CloudNativePG
            v1.15.0
        </a>
    </li>
    
</ul>

    </div>
    <div class="w-4/6 pr-20 ml-10 well hidden md:block">
        <p><h1 id="kubernetes-upgrade">Kubernetes Upgrade</h1>
<p>Kubernetes clusters must be kept updated. This becomes even more
important if you are self-managing your Kubernetes clusters, especially
on <strong>bare metal</strong>.</p>
<p>Planning and executing regular updates is a way for your organization
to clean up the technical debt and reduce the business risks, despite
the introduction in your Kubernetes infrastructure of controlled
downtimes that temporarily take out a node from the cluster for
maintenance reasons (recommended reading:
<a href="https://landing.google.com/sre/sre-book/chapters/embracing-risk/">&ldquo;Embracing Risk&rdquo;</a>
from the Site Reliability Engineering book).</p>
<p>For example, you might need to apply security updates on the Linux
servers where Kubernetes is installed, or to replace a malfunctioning
hardware component such as RAM, CPU, or RAID controller, or even upgrade
the cluster to the latest version of Kubernetes.</p>
<p>Usually, maintenance operations in a cluster are performed one node
at a time by:</p>
<ol>
<li>evicting the workloads from the node to be updated (<code>drain</code>)</li>
<li>performing the actual operation (for example, system update)</li>
<li>re-joining the node to the cluster (<code>uncordon</code>)</li>
</ol>
<p>The above process requires workloads to be either stopped for the
entire duration of the upgrade or migrated to another node.</p>
<p>While the latest case is the expected one in terms of service
reliability and self-healing capabilities of Kubernetes, there can
be situations where it is advised to operate with a temporarily
degraded cluster and wait for the upgraded node to be up again.</p>
<p>In particular, if your PostgreSQL cluster relies on <strong>node-local storage</strong>
- that is <em>storage which is local to the Kubernetes worker node where
the PostgreSQL database is running</em>.
Node-local storage (or simply <em>local storage</em>) is used to enhance performance.</p>
<p>!!! Note
If your database files are on shared storage over the network,
you may not need to define a maintenance window. If the volumes currently
used by the pods can be reused by pods running on different nodes after
the drain, the default self-healing behavior of the operator will work
fine (you can then skip the rest of this section).</p>
<p>When using local storage for PostgreSQL, you are advised to temporarily
put the cluster in <strong>maintenance mode</strong> through the <code>nodeMaintenanceWindow</code>
option to avoid standard self-healing procedures to kick in,
while, for example, enlarging the partition on the physical node or
updating the node itself.</p>
<p>!!! Warning
Limit the duration of the maintenance window to the shortest
amount of time possible. In this phase, some of the expected
behaviors of Kubernetes are either disabled or running with
some limitations, including self-healing, rolling updates,
and Pod disruption budget.</p>
<p>The <code>nodeMaintenanceWindow</code> option of the cluster has two further
settings:</p>
<p><code>inProgress</code>:
Boolean value that states if the maintenance window for the nodes
is currently in progress or not. By default, it is set to <code>off</code>.
During the maintenance window, the <code>reusePVC</code> option below is
evaluated by the operator.</p>
<p><code>reusePVC</code>:
Boolean value that defines if an existing PVC is reused or
not during the maintenance operation. By default, it is set to <code>on</code>.
When <strong>enabled</strong>, Kubernetes waits for the node to come up
again and then reuses the existing PVC; the <code>PodDisruptionBudget</code>
policy is temporarily removed.
When <strong>disabled</strong>, Kubernetes forces the recreation of the
Pod on a different node with a new PVC by relying on
PostgreSQL&rsquo;s physical streaming replication, then destroys
the old PVC together with the Pod. This scenario is generally
not recommended unless the database&rsquo;s size is small, and re-cloning
the new PostgreSQL instance takes shorter than waiting. This behavior
does <strong>not</strong> apply to clusters with only one instance and
reusePVC disabled: see section below.</p>
<p>!!! Note
When performing the <code>kubectl drain</code> command, you will need
to add the <code>--delete-local-data</code> option.
Don&rsquo;t be afraid: it refers to another volume internally used
by the operator - not the PostgreSQL data directory.</p>
<h2 id="single-instance-clusters-with-reusepvc-set-to-false">Single instance clusters with <code>reusePVC</code> set to <code>false</code></h2>
<p>!!! Important
We recommend to always create clusters with more
than one instance in order to guarantee high availability.</p>
<p>Deleting the only PostgreSQL instance in a single instance cluster with
<code>reusePVC</code> set to <code>false</code> would imply all data being lost,
therefore we prevent users from draining nodes such instances might be running
on, even in maintenance mode.</p>
<p>However, in case maintenance is required for such a node you have two options:</p>
<ol>
<li>Enable <code>reusePVC</code>, accepting the downtime</li>
<li>Replicate the instance on a different node and switch over the primary</li>
</ol>
<p>As long as a database service downtime is acceptable for your environment,
draining the node is as simple as setting the <code>nodeMaintenanceWindow</code> to
<code>inProgress: true</code> and <code>reusePVC: true</code>. This will allow the instance to
be deleted and recreated as soon as the original PVC is available
(e.g. with node local storage, as soon as the node is back up).</p>
<p>Otherwise you will have to scale up the cluster, creating a new instance
on a different node and promoting the new instance to primary in order to
shut down the original one on the node undergoing maintenance. The only
downtime in this case will be the duration of the switchover.</p>
<p>A possible approach could be:</p>
<ol>
<li>Cordon the node on which the current instance is running.</li>
<li>Scale up the cluster to 2 instances, could take some time depending on the database size.</li>
<li>As soon as the new instance is running, the operator will automatically
perform a switchover given that the current primary is running on a cordoned node.</li>
<li>Scale back down the cluster to a single instance, this will delete the old instance</li>
<li>The old primary&rsquo;s node can now be drained successfully, while leaving the new primary
running on a new node.</li>
</ol>
</p>
    </div>
    
    <div class="sticky top-0 px-4 bg-purple-2 leading-9 hidden md:hidden py-4" id="menu-list">
        
<ul>
    
    
    
    
    <li>
        <a href="/docs/v1.15.0/api_reference/">API Reference</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/applications/">Connecting from an application</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/architecture/">Architecture</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/backup_recovery/">Backup and Recovery</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/before_you_start/">Before You Start</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/bootstrap/">Bootstrap</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/certificates/">Certificates</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/cnpg-plugin/">CloudNativePG Plugin</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/commercial_support/">Commercial support</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/connection_pooling/">Connection Pooling</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/container_images/">Container Image Requirements</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/e2e/">End-to-End Tests</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/expose_pg_services/">Exposing Postgres Services</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/failover/">Automated failover</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/failure_modes/">Failure Modes</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/fencing/">Fencing</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/installation_upgrade/">Installation and upgrades</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/instance_manager/">Postgres instance manager</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/kubernetes_upgrade/">Kubernetes Upgrade</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/labels_annotations/">Labels and annotations</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/logging/">Logging</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/monitoring/">Monitoring</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/operator_capability_levels/">Operator Capability Levels</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/operator_conf/">Operator configuration</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/postgresql_conf/">PostgreSQL Configuration</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/quickstart/">Quickstart</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/release_notes/">Release notes</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/replication/">Replication</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/resource_management/">Resource management</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/rolling_update/">Rolling Updates</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/samples/">Configuration Samples</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/scheduling/">Scheduling</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/security/">Security</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/ssl_connections/">Client TLS/SSL Connections</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/storage/">Storage</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/supported_releases/">Supported releases</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/troubleshooting/">Troubleshooting</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/use_cases/">Use cases</a>
    </li>
    
    
    
</ul>

<hr class="w-1/2">


<p>Documention by version</p>
<ul>
    
    <li>
        <a href="/docs/v1.15.0/">
            CloudNativePG
            v1.15.0
        </a>
    </li>
    
</ul>

    </div>
    <div class="px-4 well md:hidden">
        <p><h1 id="kubernetes-upgrade">Kubernetes Upgrade</h1>
<p>Kubernetes clusters must be kept updated. This becomes even more
important if you are self-managing your Kubernetes clusters, especially
on <strong>bare metal</strong>.</p>
<p>Planning and executing regular updates is a way for your organization
to clean up the technical debt and reduce the business risks, despite
the introduction in your Kubernetes infrastructure of controlled
downtimes that temporarily take out a node from the cluster for
maintenance reasons (recommended reading:
<a href="https://landing.google.com/sre/sre-book/chapters/embracing-risk/">&ldquo;Embracing Risk&rdquo;</a>
from the Site Reliability Engineering book).</p>
<p>For example, you might need to apply security updates on the Linux
servers where Kubernetes is installed, or to replace a malfunctioning
hardware component such as RAM, CPU, or RAID controller, or even upgrade
the cluster to the latest version of Kubernetes.</p>
<p>Usually, maintenance operations in a cluster are performed one node
at a time by:</p>
<ol>
<li>evicting the workloads from the node to be updated (<code>drain</code>)</li>
<li>performing the actual operation (for example, system update)</li>
<li>re-joining the node to the cluster (<code>uncordon</code>)</li>
</ol>
<p>The above process requires workloads to be either stopped for the
entire duration of the upgrade or migrated to another node.</p>
<p>While the latest case is the expected one in terms of service
reliability and self-healing capabilities of Kubernetes, there can
be situations where it is advised to operate with a temporarily
degraded cluster and wait for the upgraded node to be up again.</p>
<p>In particular, if your PostgreSQL cluster relies on <strong>node-local storage</strong>
- that is <em>storage which is local to the Kubernetes worker node where
the PostgreSQL database is running</em>.
Node-local storage (or simply <em>local storage</em>) is used to enhance performance.</p>
<p>!!! Note
If your database files are on shared storage over the network,
you may not need to define a maintenance window. If the volumes currently
used by the pods can be reused by pods running on different nodes after
the drain, the default self-healing behavior of the operator will work
fine (you can then skip the rest of this section).</p>
<p>When using local storage for PostgreSQL, you are advised to temporarily
put the cluster in <strong>maintenance mode</strong> through the <code>nodeMaintenanceWindow</code>
option to avoid standard self-healing procedures to kick in,
while, for example, enlarging the partition on the physical node or
updating the node itself.</p>
<p>!!! Warning
Limit the duration of the maintenance window to the shortest
amount of time possible. In this phase, some of the expected
behaviors of Kubernetes are either disabled or running with
some limitations, including self-healing, rolling updates,
and Pod disruption budget.</p>
<p>The <code>nodeMaintenanceWindow</code> option of the cluster has two further
settings:</p>
<p><code>inProgress</code>:
Boolean value that states if the maintenance window for the nodes
is currently in progress or not. By default, it is set to <code>off</code>.
During the maintenance window, the <code>reusePVC</code> option below is
evaluated by the operator.</p>
<p><code>reusePVC</code>:
Boolean value that defines if an existing PVC is reused or
not during the maintenance operation. By default, it is set to <code>on</code>.
When <strong>enabled</strong>, Kubernetes waits for the node to come up
again and then reuses the existing PVC; the <code>PodDisruptionBudget</code>
policy is temporarily removed.
When <strong>disabled</strong>, Kubernetes forces the recreation of the
Pod on a different node with a new PVC by relying on
PostgreSQL&rsquo;s physical streaming replication, then destroys
the old PVC together with the Pod. This scenario is generally
not recommended unless the database&rsquo;s size is small, and re-cloning
the new PostgreSQL instance takes shorter than waiting. This behavior
does <strong>not</strong> apply to clusters with only one instance and
reusePVC disabled: see section below.</p>
<p>!!! Note
When performing the <code>kubectl drain</code> command, you will need
to add the <code>--delete-local-data</code> option.
Don&rsquo;t be afraid: it refers to another volume internally used
by the operator - not the PostgreSQL data directory.</p>
<h2 id="single-instance-clusters-with-reusepvc-set-to-false">Single instance clusters with <code>reusePVC</code> set to <code>false</code></h2>
<p>!!! Important
We recommend to always create clusters with more
than one instance in order to guarantee high availability.</p>
<p>Deleting the only PostgreSQL instance in a single instance cluster with
<code>reusePVC</code> set to <code>false</code> would imply all data being lost,
therefore we prevent users from draining nodes such instances might be running
on, even in maintenance mode.</p>
<p>However, in case maintenance is required for such a node you have two options:</p>
<ol>
<li>Enable <code>reusePVC</code>, accepting the downtime</li>
<li>Replicate the instance on a different node and switch over the primary</li>
</ol>
<p>As long as a database service downtime is acceptable for your environment,
draining the node is as simple as setting the <code>nodeMaintenanceWindow</code> to
<code>inProgress: true</code> and <code>reusePVC: true</code>. This will allow the instance to
be deleted and recreated as soon as the original PVC is available
(e.g. with node local storage, as soon as the node is back up).</p>
<p>Otherwise you will have to scale up the cluster, creating a new instance
on a different node and promoting the new instance to primary in order to
shut down the original one on the node undergoing maintenance. The only
downtime in this case will be the duration of the switchover.</p>
<p>A possible approach could be:</p>
<ol>
<li>Cordon the node on which the current instance is running.</li>
<li>Scale up the cluster to 2 instances, could take some time depending on the database size.</li>
<li>As soon as the new instance is running, the operator will automatically
perform a switchover given that the current primary is running on a cordoned node.</li>
<li>Scale back down the cluster to a single instance, this will delete the old instance</li>
<li>The old primary&rsquo;s node can now be drained successfully, while leaving the new primary
running on a new node.</li>
</ol>
</p>
    </div>
    
</div>

<div class="flex gap-3 bg-gray-1 px-4 py-4 cnp-blue md:hidden fixed bottom-0 w-full" id="menu-toggle">
    <img src="/icons/menu-unfold-outlined.svg" alt="">
    <h4 class="text-lg">Documentation</h4>
</div>

<script>
    const menuToggle = document.getElementById('menu-toggle')
    const menuList = document.getElementById('menu-list')
  
    menuToggle.addEventListener('click', () => {
      menuList.classList.toggle('hidden');
    })
</script>


    <footer class="text-center pb-24">
    <p class="text-sm charcoal md:w-1/2 mx-auto md:pt-10 px-6 pt-12">

      &copy; The CloudNativePG Contributors.<br />

      <a href="https://www.linuxfoundation.org/trademark-usage/"
        class="red-1">The Linux Foundation has registered trademarks and uses
        trademarks</a>.<br />

      <a href="https://www.postgresql.org/about/policies/trademarks"
        class="red-1">Postgres, PostgreSQL and the Slonik Logo are trademarks or
        registered trademarks of the PostgreSQL Community Association of Canada, and
        used with their permission</a>.

    </p>
</footer>

  </div>
  
  <script src="https://cdn.usefathom.com/script.js" data-site="GSMQCVAJ" defer></script>
  
</body>

</html>
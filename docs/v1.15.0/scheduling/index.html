<!DOCTYPE html>




<html lang="en">

<head>
  <title>CloudNative PG</title>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <meta name="description" content="CloudNativePG is the Kubernetes operator that covers the full lifecycle of a highly available PostgreSQL database cluster with a primary/standby architecture, using native streaming replication." />
  <meta name="keywords" content="" />
  <link rel="preload" href="/css/Supreme-Variable.woff2" as="font" type="font/woff2" crossorigin="anonymous" />
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet" />
  <link href="/main.min.css" rel="stylesheet" />
  <link rel="favicon" href="/favicon/favicon.ico">
  <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

<body>
  <div class="h-screen w-screen">
    
<main>
  <header>
    <div class="flex flex-row px-16 justify-between py-5">
      <div>
        
        <a href="/"><img src="/logo/large_logo.svg" class="h-14" alt="Cloud Native Postgres Logo"></a>
      </div>
      <div>
        <ul class="flex gap-14 text-2xl medium-gray my-5">
          <li><a href="/docs/1.15.0/" target="_blank">Documentation</a></li>
          <li>Blog</li>
          <li><a href="/support">Support</a></li>
          <li><a href="/end_users">End Users</a></li>
        </ul>
      </div>
      <div class="flex gap-5 my-4 fill-cnp-blue">
       <a class="github-button" href="https://github.com/cloudnative-pg/cloudnative-pg"
            data-color-scheme="no-preference: light; light: light; dark: dark;"
            data-size="large" data-show-count="true"
            aria-label="Star cloudnative-pg/cloudnative-pg on GitHub">Star</a>
        <a href="https://join.slack.com/t/cloudnativepg/shared_invite/zt-17culux7k-P_UsVOOR9teUYi4dGhDSBQ">
          <img src="/icons/Slack.svg" alt="Slack" title="Join our Slack channel now!">
        </a>
        <a href="https://twitter.com/CloudNativePg">
          <img src="/icons/Twitter.svg" alt="Twitter">
        </a>
        <a href="https://www.youtube.com/channel/UCTGH88W1BiuRRPTzJUDPJyA">
          <img src="/icons/YouTube.svg" alt="YouTube">
        </a>
      </div>
    </div>
  </header>
</main>


    
<base href="..">
<div class="px-56 pt-20 flex">
    <div class="w-1/4 px-5 bg-purple-2 pt-14 leading-9">
        
<ul>
    
    
    
    
    <li>
        <a href="/docs/v1.15.0/api_reference/">API Reference</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/applications/">Connecting from an application</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/architecture/">Architecture</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/backup_recovery/">Backup and Recovery</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/before_you_start/">Before You Start</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/bootstrap/">Bootstrap</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/certificates/">Certificates</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/cnpg-plugin/">CloudNativePG Plugin</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/commercial_support/">Commercial support</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/connection_pooling/">Connection Pooling</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/container_images/">Container Image Requirements</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/e2e/">End-to-End Tests</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/expose_pg_services/">Exposing Postgres Services</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/failover/">Automated failover</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/failure_modes/">Failure Modes</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/fencing/">Fencing</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/installation_upgrade/">Installation and upgrades</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/instance_manager/">Postgres instance manager</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/kubernetes_upgrade/">Kubernetes Upgrade</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/labels_annotations/">Labels and annotations</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/logging/">Logging</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/monitoring/">Monitoring</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/operator_capability_levels/">Operator Capability Levels</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/operator_conf/">Operator configuration</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/postgresql_conf/">PostgreSQL Configuration</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/quickstart/">Quickstart</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/release_notes/">Release notes</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/replication/">Replication</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/resource_management/">Resource management</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/rolling_update/">Rolling Updates</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/samples/">Configuration Samples</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/scheduling/">Scheduling</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/security/">Security</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/ssl_connections/">Client TLS/SSL Connections</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/storage/">Storage</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/supported_releases/">Supported releases</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/troubleshooting/">Troubleshooting</a>
    </li>
    
    
    
    <li>
        <a href="/docs/v1.15.0/use_cases/">Use cases</a>
    </li>
    
    
    
</ul>

<hr class="w-1/2">


<p>Documention by version</p>
<ul>
    
    <li>
        <a href="/docs/v1.15.0/">
            CloudNativePG
            v1.15.0
        </a>
    </li>
    
</ul>

    </div>
    <div class="w-4/6 pr-20 ml-10 well">
        <p><h1 id="scheduling">Scheduling</h1>
<p>Scheduling, in Kubernetes, is the process responsible for placing a new pod on
the best node possible, based on several criteria.</p>
<p>!!! Seealso &ldquo;Kubernetes documentation&rdquo;
Please refer to the
<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/">Kubernetes documentation</a>
for more information on scheduling, including all the available policies. On
this page we assume you are familiar with concepts like affinity,
anti-affinity, node selectors, and so on.</p>
<p>You can control how the CloudNativePG cluster&rsquo;s instances should be
scheduled through the <a href="api_reference.md#AffinityConfiguration"><code>affinity</code></a>
section in the definition of the cluster, which supports:</p>
<ul>
<li>pod affinity/anti-affinity</li>
<li>node selectors</li>
<li>tolerations</li>
</ul>
<p>!!! Info
CloudNativePG does not support pod templates for finer control
on the scheduling of workloads. While they were part of the initial concept,
the development team decided to postpone their introduction in a newer
version of the API (most likely v2 of CNPG).</p>
<h2 id="pod-affinity-and-anti-affinity">Pod affinity and anti-affinity</h2>
<p>Kubernetes allows you to control which nodes a pod should (<em>affinity</em>) or
should not (<em>anti-affinity</em>) be scheduled, based on the actual workloads already
running in those nodes.
This is technically known as <strong>inter-pod affinity/anti-affinity</strong>.</p>
<p>CloudNativePG by default will configure the cluster&rsquo;s instances
preferably on different nodes, resulting in the following <code>affinity</code> definition:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">affinity</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">podAntiAffinity</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">preferredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">podAffinityTerm</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">labelSelector</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">matchExpressions</span>:
</span></span><span style="display:flex;"><span>              - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">postgresql</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">In</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">values</span>:
</span></span><span style="display:flex;"><span>                  - <span style="color:#ae81ff">cluster-example</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">topologyKey</span>: <span style="color:#ae81ff">kubernetes.io/hostname</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">weight</span>: <span style="color:#ae81ff">100</span>
</span></span></code></pre></div><p>As a result of the following Cluster spec:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">postgresql.cnpg.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Cluster</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-example</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">instances</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">imageName</span>: <span style="color:#ae81ff">ghcr.io/cloudnative-pg/postgresql:14.2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">affinity</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enablePodAntiAffinity</span>: <span style="color:#66d9ef">true</span> <span style="color:#75715e">#default value</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">topologyKey</span>: <span style="color:#ae81ff">kubernetes.io/hostname</span> <span style="color:#75715e">#defaul value</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">podAntiAffinityType</span>: <span style="color:#ae81ff">preferred</span> <span style="color:#75715e">#default value</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storage</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">size</span>: <span style="color:#ae81ff">1Gi</span>
</span></span></code></pre></div><p>Therefore, Kubernetes will <em>prefer</em> to schedule a 3-node PostgreSQL cluster over 3
different nodes - resources permitting.</p>
<p>The aforementioned default behavior can be changed by tweaking the above settings.</p>
<p><code>podAntiAffinityType</code> can be set to <code>required</code>: resulting in
<code>requiredDuringSchedulingIgnoredDuringExecution</code> being used instead of
<code>preferredDuringSchedulingIgnoredDuringExecution</code>. Please, be aware that such a
strong requirement might result in pending instances in case resources are not
available (which is an expected condition when using
<a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Cluster Autoscaler</a> <!-- raw HTML omitted -->
for automated horizontal scaling of a Kubernetes cluster).</p>
<p>!!! Seealso &ldquo;Inter-pod affinity and anti-affinity&rdquo;
More information on this topic is in the
<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">Kubernetes documentation</a>.</p>
<p>Another possible value for <code>topologyKey</code> in a cloud environment can be
<code>topology.kubernetes.io/zone</code>, to be sure pods will be spread across
availability zones and not just nodes.  Please refer to
<a href="https://kubernetes.io/docs/reference/labels-annotations-taints/">&ldquo;Well-Known Labels, Annotations and Taints&rdquo;</a>
for more options.</p>
<p>You can disable the operator&rsquo;s generated anti-affinity policies by setting
<code>enablePodAntiAffinity</code> to false.</p>
<p>Additionally, in case a more fine-grained control is needed, you can specify a
list of custom pod affinity or anti-affinity rules via the
<code>additionalPodAffinity</code> and <code>additionalPodAntiAffinity</code> configuration
attributes. These rules will be added to the ones generated by the operator,
if enabled, or passed transparently otherwise.</p>
<p>!!! Note
You have to pass to <code>additionalPodAntiAffinity</code> or <code>additionalPodAffinity</code>
the whole content of <code>podAntiAffinity</code> or <code>podAffinity</code> that is expected by the
Pod spec (please look at the following YAML as an example of having only one
instance of PostgreSQL running on every worker node, regardless of which
PostgreSQL cluster they belong to).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>    <span style="color:#f92672">additionalPodAntiAffinity</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">labelSelector</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">matchExpressions</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">postgresql</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">Exists</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">values</span>: []
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">topologyKey</span>: <span style="color:#e6db74">&#34;kubernetes.io/hostname&#34;</span>
</span></span></code></pre></div><h2 id="node-selection-through-nodeselector">Node selection through <code>nodeSelector</code></h2>
<p>Kubernetes allows <code>nodeSelector</code> to provide a list of labels (defined as
key-value pairs) to select the nodes on which a pod can run. Specifically,
the node must have each indicated key-value pair as labels for the
pod to be scheduled and run.</p>
<p>Similarly, CloudNativePG consents you to define a <code>nodeSelector</code> in the
<code>affinity</code> section, so that you can request a PostgreSQL cluster to run only
on nodes that have those labels.</p>
<h2 id="tolerations">Tolerations</h2>
<p>Kubernetes allows you to specify (through <code>taints</code>) whether a node should repel
all pods not explicitly tolerating (through <code>tolerations</code>) their <code>taints</code>.</p>
<p>So, by setting a proper set of <code>tolerations</code> for a workload matching a specific
node&rsquo;s <code>taints</code>, Kubernetes scheduler will now take into consideration the
tainted node, while deciding on which node to schedule the workload.
Tolerations can be configured for all the pods of a Cluster through the
<code>.spec.affinity.tolerations</code> section, which accepts the usual Kubernetes syntax
for tolerations.</p>
<p>!!! Seealso &ldquo;Taints and Tolerations&rdquo;
More information on taints and tolerations can be found in the
<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Kubernetes documentation</a>.</p>
</p>
    </div>
</div>


    <footer class="text-center pb-24">
    <p class="text-sm charcoal w-1/2 mx-auto pt-10">&copy; 2019-2022 The CloudNativePG Contributors. All rights
        reserved. The Linux Foundation has registered trademarks and uses trademarks.
        For a list of trademarks of The Linux Foundation, please see our <a href="#" class="red-1">Trademark Usage
            page</a>.</p>
</footer>
  </div>
  
  <script src="https://cdn.usefathom.com/script.js" data-site="GSMQCVAJ" defer></script>
  
</body>

</html>
